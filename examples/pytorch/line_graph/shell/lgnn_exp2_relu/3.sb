#!/bin/bash
#SBATCH --verbose
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
#SBATCH --time=4:00:00
#SBATCH --mem=64GB
#SBATCH --job-name=exp2_relu_3
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=leic.monitor@gmail.com
#SBATCH --output=hpc_outputs/slurm_%j.out
#SBATCH --error=hpc_outputs/slurm_%j.err 
#SBATCH --partition=v100_sxm2_4,v100_pci_2,p40_4
#SBATCH --gres=gpu:1

cd /home/lc3909/dgl/examples/pytorch/line_graph
module purge

source /home/lc3909/anaconda3/bin/activate galaxy1

set -x

model_name=lgnn_naive_sharing_relu_3

'''
python train_lgnn_naive_sharing_relu.py \
--n-nodes 400 \
--n-graphs 15000 \
--n-layers 3 \
--gpu 0 \
--interval 100 \
--verbose \
--save-path ${model_name} 
'''

for layers in 90 120 
do
python test_lgnn_naive_sharing_relu.py \
--gpu 0 \
--n-nodes 400 \
--n-layers ${layers} \
--n-graphs 300 \
--model-path ${model_name}
done
