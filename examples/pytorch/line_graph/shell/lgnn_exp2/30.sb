#!/bin/bash
#SBATCH --verbose
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
#SBATCH --time=10:00:00
#SBATCH --mem=64GB
#SBATCH --job-name=exp2_30
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=leic.monitor@gmail.com
#SBATCH --output=hpc_outputs/slurm_%j.out
#SBATCH --error=hpc_outputs/slurm_%j.err 
#SBATCH --gres=gpu:1

cd /home/lc3909/dgl/examples/pytorch/line_graph
module purge

source /home/lc3909/anaconda3/bin/activate galaxy1

set -x

model_name=lgnn_naive_sharing_30

python train_lgnn_naive_sharing.py \
--n-nodes 400 \
--n-graphs 15000 \
--n-layers 30 \
--gpu 0 \
--interval 100 \
--verbose \

for layers in 3 5 10 20 30 60 90 120 
do
python test_lgnn_naive_sharing.py \
--gpu 0 \
--n-nodes 400 \
--n-layers ${layers} \
--n-graphs 300 \
--model-path ${model_name}
done
