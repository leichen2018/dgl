#!/bin/bash
#SBATCH --verbose
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
#SBATCH --time=18:00:00
#SBATCH --mem=64GB
#SBATCH --job-name=cls2000_60000
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=leic.monitor@gmail.com
#SBATCH --output=hpc_outputs/slurm_%j.out
#SBATCH --error=hpc_outputs/slurm_%j.err 
#SBATCH --partition=v100_sxm2_4,v100_pci_2,p40_4
#SBATCH --gres=gpu:1

cd /home/lc3909/dgl/examples/pytorch/line_graph
module purge

source /home/lc3909/anaconda3/bin/activate galaxy1

set -x

python train_mutual.py \
--n-nodes 400 \
--n-graphs 60000 \
--gpu 0 \
--n-clusters 2000 \
--interval 100 \
--verbose \
--save-path mutual_2000_60000 

